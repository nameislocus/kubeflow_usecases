{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.8/site-packages (1.19.5)\n",
      "Requirement already satisfied: boto3 in /opt/app-root/lib/python3.8/site-packages (1.17.11)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/app-root/lib/python3.8/site-packages (from boto3) (0.3.7)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.11 in /opt/app-root/lib/python3.8/site-packages (from boto3) (1.20.95)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/app-root/lib/python3.8/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/app-root/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.11->boto3) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/app-root/lib/python3.8/site-packages (from botocore<1.21.0,>=1.20.11->boto3) (1.26.5)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.11->boto3) (1.15.0)\n",
      "Requirement already satisfied: torch in /opt/app-root/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/app-root/lib/python3.8/site-packages (from torch) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install boto3\n",
    "!pip install torch\n",
    "import numpy as np\n",
    "import utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import boto3\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_IMAGE = 'docker.io/pytorch/pytorch:1.0-cuda10.0-cudnn7-runtime'\n",
    "bucket_name = 'elyra'\n",
    "S3_END_POINT = \"http://minio-kubeflow.apps.openmaru.ocp482.com\"\n",
    "S3_ACCESS_ID = \"minio\"\n",
    "S3_ACCESS_KEY = \"minio123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Name': 'elyra',\n",
       "  'CreationDate': datetime.datetime(2021, 8, 15, 12, 40, 14, 518000, tzinfo=tzlocal())},\n",
       " {'Name': 'mlpipeline',\n",
       "  'CreationDate': datetime.datetime(2021, 7, 29, 13, 47, 55, 26000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_endpoint_url = 'http://minio-kubeflow.apps.openmaru.ocp482.com'\n",
    "s3_access_key = \"minio\"\n",
    "s3_secret_key = \"minio123\"\n",
    "\n",
    "s3 = boto3.client(service_name='s3',\n",
    "              \taws_access_key_id = s3_access_key,\n",
    "              \taws_secret_access_key = s3_secret_key,\n",
    "              \tendpoint_url=s3_endpoint_url)\n",
    "\n",
    "s3.list_buckets()['Buckets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, n_inputs=2, n_outputs=1, n_hidden_nodes=10, n_hidden_layers=1, activation=nn.ReLU(), output_activation=None):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.layer_list = nn.ModuleList()\n",
    "\n",
    "        for i in range(n_hidden_layers):\n",
    "            if i==0:\n",
    "                self.layer_list.append(nn.Linear(n_inputs, n_hidden_nodes))\n",
    "            else:\n",
    "                self.layer_list.append(nn.Linear(n_hidden_nodes, n_hidden_nodes))\n",
    "        \n",
    "        self.output_layer = nn.Linear(n_hidden_nodes, n_outputs)\n",
    "\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "\n",
    "        for layer in self.layer_list:\n",
    "            out = self.activation(layer(out))\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "        if self.output_activation is not None:\n",
    "            out = self.output_activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_store(bucket, data, key):\n",
    "    s3.put_object(Body=pickle.dumps(data),\n",
    "                      Bucket=bucket,\n",
    "                      Key=key)\n",
    "    \n",
    "def read_from_store(bucket, key):\n",
    "    raw_data = s3.get_object(Bucket=bucket,\n",
    "                                 Key=key)['Body']._raw_stream.data\n",
    "\n",
    "    return pickle.loads(raw_data)\n",
    "\n",
    "def train_model(hyperparam_idx: int, retcode_download: int, N_gridsize: int) -> int:\n",
    "    '''Look up hyperparams from store\n",
    "    and train model\n",
    "    '''\n",
    "\n",
    "    if hyperparam_idx >= N_gridsize:\n",
    "        raise ValueError(\"hyperparam_idx cannot be >= N_gridsize\")\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Device = {device}')\n",
    "\n",
    "\n",
    "    features_train = torch.from_numpy(read_from_store(bucket_name, 'features_train')).float()\n",
    "    target_train = torch.from_numpy(read_from_store(bucket_name, 'target_train')).float()\n",
    "    features_test = torch.from_numpy(read_from_store(bucket_name, 'features_test')).float()\n",
    "    target_test = torch.from_numpy(read_from_store(bucket_name, 'target_test')).float()\n",
    "\n",
    "    conf = read_from_store(bucket_name, 'hyperparam_grid')[hyperparam_idx]\n",
    "    lr = float(conf.get('lr', 1e-2))\n",
    "    N_epochs = int(conf.get('N_epochs', 1000))\n",
    "    num_hidden_layers = int(conf.get('num_hidden_layers', 1))\n",
    "    num_nodes = int(conf.get('num_nodes', 2))\n",
    "    activation = conf.get('activation', 'relu')\n",
    "\n",
    "    #should be dependent on vars read from config\n",
    "    if activation=='relu':\n",
    "        activation = nn.ReLU()\n",
    "    elif activation=='sigmoid':\n",
    "        activation = nn.Sigmoid()\n",
    "\n",
    "    model = Net(n_inputs=2, n_outputs=1, n_hidden_nodes=num_nodes, n_hidden_layers=num_hidden_layers, activation=activation, output_activation=nn.Sigmoid())\n",
    "    #model = nn.Sequential(nn.Linear(2, 10), nn.ReLU(), nn.Linear(10, 1), nn.Sigmoid())\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) #Adam optimizer\n",
    "    model.train()    \n",
    "\n",
    "    if device!='cpu':\n",
    "        model = model.to(device)\n",
    "        features_train = features_train.to(device)\n",
    "        target_train = target_train.to(device)\n",
    "\n",
    "    for epoch in range(N_epochs): #N_epochs = number of iterations over the full dataset\n",
    "        features_shuffled = features_train\n",
    "        target_shuffled = target_train\n",
    "\n",
    "        out = model(features_shuffled) #predictions from model\n",
    "        loss = criterion(out.squeeze(), target_shuffled.squeeze()) #loss between predictions and labels\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f'epoch = {epoch} loss = {loss}')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #compute gradients\n",
    "        optimizer.step() #update model\n",
    "\n",
    "    out = model(features_shuffled) #predictions from model\n",
    "    train_loss = criterion(out.squeeze(), target_shuffled.squeeze()) #loss between predictions and labels\n",
    "    print(f'Train Loss : {train_loss}')\n",
    "\n",
    "    def evaluate_model(model, features_test, target_test):\n",
    "        '''Evaluate model on test set\n",
    "        and store result\n",
    "        '''\n",
    "        model.eval()\n",
    "\n",
    "        if device!='cpu':\n",
    "            features_test = features_test.to(device)\n",
    "            target_test = target_test.to(device)\n",
    "\n",
    "        out = model(features_test)\n",
    "        loss = criterion(out.squeeze(), target_test.squeeze())\n",
    "        \n",
    "\n",
    "        return loss\n",
    "\n",
    "    test_loss = evaluate_model(model, features_test, target_test)\n",
    "    print(f'Test  Loss : {test_loss}')\n",
    "\n",
    "    #write_to_store(bucket_name, {'test_loss': test_loss.item(), 'model': model}, f'score_{hyperparam_idx}', client)\n",
    "    write_to_store(bucket_name, {'test_loss': test_loss.item()}, f'score_{hyperparam_idx}')\n",
    "\n",
    "    return hyperparam_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device = cpu\n",
      "epoch = 0 loss = 0.7473655343055725\n",
      "Train Loss : 0.6931474208831787\n",
      "Test  Loss : 0.6931473612785339\n"
     ]
    }
   ],
   "source": [
    "retcode_download = 0\n",
    "N_gridsize = 18\n",
    "hyperparam_idx = 0 #this should be a notebook parameter\n",
    "\n",
    "retcode = train_model(hyperparam_idx, retcode_download, N_gridsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
