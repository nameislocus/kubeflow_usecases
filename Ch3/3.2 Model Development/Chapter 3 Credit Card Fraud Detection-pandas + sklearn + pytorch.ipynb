{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version:\n",
    "* Reads data from a CSV file using pandas (Spark to come later)\n",
    "* Does no preprocessing (where Spark should really help for large datasets\n",
    "* Doesn't do anything about class imbalance except to look at recall and precision\n",
    "* Trains both a sklearn (random forest) and a neural network (PyTorch) model\n",
    "* Is not how a data scientist at a bank would build a statistically robust model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/app-root/lib/python3.8/site-packages (21.2.4)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.19.5)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.25.1)\n",
      "Requirement already satisfied: Flask in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.1.4)\n",
      "Requirement already satisfied: Pillow in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (8.2.0)\n",
      "Requirement already satisfied: tensorflow in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (2.6.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.2.4)\n",
      "Requirement already satisfied: matplotlib in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.1.3)\n",
      "Requirement already satisfied: jupyter in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: torch in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.9.0)\n",
      "Requirement already satisfied: torchvision in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (0.10.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 12)) (0.24.2)\n",
      "Requirement already satisfied: scipy in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (1.6.3)\n",
      "Requirement already satisfied: sklearn in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 14)) (0.0)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: jinja2==2.11.3 in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (2.11.3)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.3 in /opt/app-root/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (4.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/app-root/lib/python3.8/site-packages (from jinja2==2.11.3->-r requirements.txt (line 16)) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/app-root/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/app-root/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/app-root/lib/python3.8/site-packages (from requests->-r requirements.txt (line 2)) (1.26.5)\n",
      "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /opt/app-root/lib/python3.8/site-packages (from Flask->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /opt/app-root/lib/python3.8/site-packages (from Flask->-r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: click<8.0,>=5.1 in /opt/app-root/lib/python3.8/site-packages (from Flask->-r requirements.txt (line 3)) (7.1.2)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.39.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (0.13.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (3.3.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (2.6.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (2.6.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.6.3)\n",
      "Requirement already satisfied: gast==0.4.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (0.4.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (0.36.2)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.2)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (3.17.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.12)\n",
      "Requirement already satisfied: clang~=5.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (5.0)\n",
      "Requirement already satisfied: keras~=2.6 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (2.6.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (3.7.4.3)\n",
      "Requirement already satisfied: six~=1.15.0 in /opt/app-root/lib/python3.8/site-packages (from tensorflow->-r requirements.txt (line 5)) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/app-root/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/app-root/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: notebook in /opt/app-root/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 8)) (6.4.0)\n",
      "Requirement already satisfied: ipykernel in /opt/app-root/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 8)) (5.5.5)\n",
      "Requirement already satisfied: qtconsole in /opt/app-root/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 8)) (5.1.1)\n",
      "Requirement already satisfied: jupyter-console in /opt/app-root/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 8)) (6.4.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/app-root/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 8)) (7.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/app-root/lib/python3.8/site-packages (from jupyter->-r requirements.txt (line 8)) (5.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 12)) (2.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (1.31.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (0.4.5)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/app-root/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (57.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/app-root/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /opt/app-root/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (7.24.1)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /opt/app-root/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (5.0.5)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/app-root/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /opt/app-root/lib/python3.8/site-packages (from ipykernel->jupyter->-r requirements.txt (line 8)) (6.1.12)\n",
      "Requirement already satisfied: decorator in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.7.5)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.1.2)\n",
      "Requirement already satisfied: pygments in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (2.9.0)\n",
      "Requirement already satisfied: backcall in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.17.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (3.0.18)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/app-root/lib/python3.8/site-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /opt/app-root/lib/python3.8/site-packages (from traitlets>=4.1.0->ipykernel->jupyter->-r requirements.txt (line 8)) (0.2.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/app-root/lib/python3.8/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (1.0.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/app-root/lib/python3.8/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (5.1.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/app-root/lib/python3.8/site-packages (from ipywidgets->jupyter->-r requirements.txt (line 8)) (3.5.1)\n",
      "Requirement already satisfied: jupyter-core in /opt/app-root/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 8)) (4.7.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/app-root/lib/python3.8/site-packages (from nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 8)) (3.2.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/app-root/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 8)) (21.2.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/app-root/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets->jupyter->-r requirements.txt (line 8)) (0.17.3)\n",
      "Requirement already satisfied: prometheus-client in /opt/app-root/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.11.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/app-root/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (22.1.0)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/app-root/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (1.5.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/app-root/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (0.10.1)\n",
      "Requirement already satisfied: argon2-cffi in /opt/app-root/lib/python3.8/site-packages (from notebook->jupyter->-r requirements.txt (line 8)) (20.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/app-root/lib/python3.8/site-packages (from argon2-cffi->notebook->jupyter->-r requirements.txt (line 8)) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/app-root/lib/python3.8/site-packages (from cffi>=1.0.0->argon2-cffi->notebook->jupyter->-r requirements.txt (line 8)) (2.20)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/app-root/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (1.4.3)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/app-root/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/app-root/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /opt/app-root/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.7.1)\n",
      "Requirement already satisfied: testpath in /opt/app-root/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.0)\n",
      "Requirement already satisfied: bleach in /opt/app-root/lib/python3.8/site-packages (from nbconvert->jupyter->-r requirements.txt (line 8)) (3.3.0)\n",
      "Requirement already satisfied: webencodings in /opt/app-root/lib/python3.8/site-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (0.5.1)\n",
      "Requirement already satisfied: packaging in /opt/app-root/lib/python3.8/site-packages (from bleach->nbconvert->jupyter->-r requirements.txt (line 8)) (20.9)\n",
      "Requirement already satisfied: qtpy in /opt/app-root/lib/python3.8/site-packages (from qtconsole->jupyter->-r requirements.txt (line 8)) (1.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "#import sklearn.external as joblib\n",
    "#import sklearn.external.joblib as extjoblib\n",
    "#import joblib\n",
    "#import sklearn.external.joblib as extjoblib\n",
    "import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion and Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142404, 31)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118850.0</td>\n",
       "      <td>-0.900623</td>\n",
       "      <td>0.280588</td>\n",
       "      <td>-1.164185</td>\n",
       "      <td>-0.479060</td>\n",
       "      <td>2.306457</td>\n",
       "      <td>-1.109820</td>\n",
       "      <td>2.091163</td>\n",
       "      <td>-1.014135</td>\n",
       "      <td>-0.163032</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028738</td>\n",
       "      <td>0.703406</td>\n",
       "      <td>0.077963</td>\n",
       "      <td>0.410507</td>\n",
       "      <td>-0.203358</td>\n",
       "      <td>0.492114</td>\n",
       "      <td>-0.526242</td>\n",
       "      <td>-0.085891</td>\n",
       "      <td>62.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>61914.0</td>\n",
       "      <td>1.090784</td>\n",
       "      <td>-0.248195</td>\n",
       "      <td>0.569939</td>\n",
       "      <td>0.751200</td>\n",
       "      <td>-0.623526</td>\n",
       "      <td>-0.213400</td>\n",
       "      <td>-0.210090</td>\n",
       "      <td>0.086138</td>\n",
       "      <td>0.700659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258564</td>\n",
       "      <td>-0.704155</td>\n",
       "      <td>0.081999</td>\n",
       "      <td>0.069435</td>\n",
       "      <td>0.186789</td>\n",
       "      <td>0.314808</td>\n",
       "      <td>-0.022921</td>\n",
       "      <td>0.018028</td>\n",
       "      <td>54.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39414.0</td>\n",
       "      <td>1.123039</td>\n",
       "      <td>-0.283453</td>\n",
       "      <td>0.140288</td>\n",
       "      <td>-0.016770</td>\n",
       "      <td>-0.485220</td>\n",
       "      <td>-0.549257</td>\n",
       "      <td>-0.068658</td>\n",
       "      <td>-0.070406</td>\n",
       "      <td>0.013267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136039</td>\n",
       "      <td>0.223308</td>\n",
       "      <td>-0.190277</td>\n",
       "      <td>0.095916</td>\n",
       "      <td>0.383227</td>\n",
       "      <td>1.110139</td>\n",
       "      <td>-0.101643</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>81.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136181.0</td>\n",
       "      <td>1.979781</td>\n",
       "      <td>-0.140372</td>\n",
       "      <td>-3.090830</td>\n",
       "      <td>0.271226</td>\n",
       "      <td>2.915221</td>\n",
       "      <td>3.309011</td>\n",
       "      <td>0.035839</td>\n",
       "      <td>0.667343</td>\n",
       "      <td>0.042450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055994</td>\n",
       "      <td>0.213071</td>\n",
       "      <td>0.010282</td>\n",
       "      <td>0.708561</td>\n",
       "      <td>0.502361</td>\n",
       "      <td>-0.474254</td>\n",
       "      <td>-0.002895</td>\n",
       "      <td>-0.064011</td>\n",
       "      <td>31.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>133984.0</td>\n",
       "      <td>2.335354</td>\n",
       "      <td>-1.095306</td>\n",
       "      <td>-1.259527</td>\n",
       "      <td>-1.552168</td>\n",
       "      <td>-1.029081</td>\n",
       "      <td>-1.374204</td>\n",
       "      <td>-0.642418</td>\n",
       "      <td>-0.512668</td>\n",
       "      <td>-1.747542</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084147</td>\n",
       "      <td>0.789308</td>\n",
       "      <td>0.056403</td>\n",
       "      <td>0.093860</td>\n",
       "      <td>0.128412</td>\n",
       "      <td>0.064020</td>\n",
       "      <td>-0.006132</td>\n",
       "      <td>-0.058280</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0  118850.0 -0.900623  0.280588 -1.164185 -0.479060  2.306457 -1.109820   \n",
       "1   61914.0  1.090784 -0.248195  0.569939  0.751200 -0.623526 -0.213400   \n",
       "2   39414.0  1.123039 -0.283453  0.140288 -0.016770 -0.485220 -0.549257   \n",
       "3  136181.0  1.979781 -0.140372 -3.090830  0.271226  2.915221  3.309011   \n",
       "4  133984.0  2.335354 -1.095306 -1.259527 -1.552168 -1.029081 -1.374204   \n",
       "\n",
       "         V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
       "0  2.091163 -1.014135 -0.163032  ... -0.028738  0.703406  0.077963  0.410507   \n",
       "1 -0.210090  0.086138  0.700659  ... -0.258564 -0.704155  0.081999  0.069435   \n",
       "2 -0.068658 -0.070406  0.013267  ...  0.136039  0.223308 -0.190277  0.095916   \n",
       "3  0.035839  0.667343  0.042450  ...  0.055994  0.213071  0.010282  0.708561   \n",
       "4 -0.642418 -0.512668 -1.747542  ...  0.084147  0.789308  0.056403  0.093860   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0 -0.203358  0.492114 -0.526242 -0.085891   62.34      0  \n",
       "1  0.186789  0.314808 -0.022921  0.018028   54.52      0  \n",
       "2  0.383227  1.110139 -0.101643  0.000761   81.23      0  \n",
       "3  0.502361 -0.474254 -0.002895 -0.064011   31.65      0  \n",
       "4  0.128412  0.064020 -0.006132 -0.058280   15.00      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of labels: Extreme class imbalance\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    142166\n",
       "1       238\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Distribution of labels: Extreme class imbalance')\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd10f851e20>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "      <td>142404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94946.248273</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>-0.002674</td>\n",
       "      <td>-0.003948</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>-0.005787</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>-0.000192</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>-0.001171</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>88.454103</td>\n",
       "      <td>0.001671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47465.960339</td>\n",
       "      <td>1.949620</td>\n",
       "      <td>1.652243</td>\n",
       "      <td>1.508556</td>\n",
       "      <td>1.413065</td>\n",
       "      <td>1.374147</td>\n",
       "      <td>1.329253</td>\n",
       "      <td>1.217438</td>\n",
       "      <td>1.187849</td>\n",
       "      <td>1.101259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.723052</td>\n",
       "      <td>0.725377</td>\n",
       "      <td>0.613224</td>\n",
       "      <td>0.603888</td>\n",
       "      <td>0.521120</td>\n",
       "      <td>0.481864</td>\n",
       "      <td>0.399919</td>\n",
       "      <td>0.318633</td>\n",
       "      <td>246.907421</td>\n",
       "      <td>0.040848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-56.407510</td>\n",
       "      <td>-72.715728</td>\n",
       "      <td>-33.680984</td>\n",
       "      <td>-5.519697</td>\n",
       "      <td>-42.147898</td>\n",
       "      <td>-26.160506</td>\n",
       "      <td>-41.506796</td>\n",
       "      <td>-50.420090</td>\n",
       "      <td>-13.434066</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.889347</td>\n",
       "      <td>-9.499423</td>\n",
       "      <td>-32.828995</td>\n",
       "      <td>-2.836627</td>\n",
       "      <td>-8.696627</td>\n",
       "      <td>-2.604551</td>\n",
       "      <td>-22.565679</td>\n",
       "      <td>-11.710896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54372.750000</td>\n",
       "      <td>-0.921933</td>\n",
       "      <td>-0.601028</td>\n",
       "      <td>-0.895517</td>\n",
       "      <td>-0.849840</td>\n",
       "      <td>-0.694568</td>\n",
       "      <td>-0.771513</td>\n",
       "      <td>-0.554193</td>\n",
       "      <td>-0.209864</td>\n",
       "      <td>-0.641984</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227556</td>\n",
       "      <td>-0.543302</td>\n",
       "      <td>-0.162090</td>\n",
       "      <td>-0.353028</td>\n",
       "      <td>-0.317476</td>\n",
       "      <td>-0.327046</td>\n",
       "      <td>-0.071220</td>\n",
       "      <td>-0.053085</td>\n",
       "      <td>5.630000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84934.000000</td>\n",
       "      <td>0.013515</td>\n",
       "      <td>0.065422</td>\n",
       "      <td>0.176833</td>\n",
       "      <td>-0.023964</td>\n",
       "      <td>-0.052936</td>\n",
       "      <td>-0.279320</td>\n",
       "      <td>0.041810</td>\n",
       "      <td>0.021851</td>\n",
       "      <td>-0.051165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029045</td>\n",
       "      <td>0.008549</td>\n",
       "      <td>-0.011382</td>\n",
       "      <td>0.040610</td>\n",
       "      <td>0.014350</td>\n",
       "      <td>-0.053482</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.011287</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139373.500000</td>\n",
       "      <td>1.317522</td>\n",
       "      <td>0.808003</td>\n",
       "      <td>1.024905</td>\n",
       "      <td>0.734927</td>\n",
       "      <td>0.615795</td>\n",
       "      <td>0.392253</td>\n",
       "      <td>0.573250</td>\n",
       "      <td>0.327794</td>\n",
       "      <td>0.599410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186741</td>\n",
       "      <td>0.531624</td>\n",
       "      <td>0.148334</td>\n",
       "      <td>0.438298</td>\n",
       "      <td>0.349133</td>\n",
       "      <td>0.240137</td>\n",
       "      <td>0.090985</td>\n",
       "      <td>0.078742</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172788.000000</td>\n",
       "      <td>2.454930</td>\n",
       "      <td>21.467203</td>\n",
       "      <td>9.382558</td>\n",
       "      <td>16.875344</td>\n",
       "      <td>34.801666</td>\n",
       "      <td>23.917837</td>\n",
       "      <td>44.054461</td>\n",
       "      <td>20.007208</td>\n",
       "      <td>15.594995</td>\n",
       "      <td>...</td>\n",
       "      <td>27.202839</td>\n",
       "      <td>8.361985</td>\n",
       "      <td>22.083545</td>\n",
       "      <td>4.016342</td>\n",
       "      <td>6.070850</td>\n",
       "      <td>3.517346</td>\n",
       "      <td>12.152401</td>\n",
       "      <td>33.847808</td>\n",
       "      <td>18910.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time             V1             V2             V3  \\\n",
       "count  142404.000000  142404.000000  142404.000000  142404.000000   \n",
       "mean    94946.248273      -0.000148      -0.000094      -0.002674   \n",
       "std     47465.960339       1.949620       1.652243       1.508556   \n",
       "min         0.000000     -56.407510     -72.715728     -33.680984   \n",
       "25%     54372.750000      -0.921933      -0.601028      -0.895517   \n",
       "50%     84934.000000       0.013515       0.065422       0.176833   \n",
       "75%    139373.500000       1.317522       0.808003       1.024905   \n",
       "max    172788.000000       2.454930      21.467203       9.382558   \n",
       "\n",
       "                  V4             V5             V6             V7  \\\n",
       "count  142404.000000  142404.000000  142404.000000  142404.000000   \n",
       "mean       -0.003948       0.001780      -0.005787       0.001443   \n",
       "std         1.413065       1.374147       1.329253       1.217438   \n",
       "min        -5.519697     -42.147898     -26.160506     -41.506796   \n",
       "25%        -0.849840      -0.694568      -0.771513      -0.554193   \n",
       "50%        -0.023964      -0.052936      -0.279320       0.041810   \n",
       "75%         0.734927       0.615795       0.392253       0.573250   \n",
       "max        16.875344      34.801666      23.917837      44.054461   \n",
       "\n",
       "                  V8             V9  ...            V21            V22  \\\n",
       "count  142404.000000  142404.000000  ...  142404.000000  142404.000000   \n",
       "mean       -0.000192       0.000049  ...       0.000673       0.001345   \n",
       "std         1.187849       1.101259  ...       0.723052       0.725377   \n",
       "min       -50.420090     -13.434066  ...     -22.889347      -9.499423   \n",
       "25%        -0.209864      -0.641984  ...      -0.227556      -0.543302   \n",
       "50%         0.021851      -0.051165  ...      -0.029045       0.008549   \n",
       "75%         0.327794       0.599410  ...       0.186741       0.531624   \n",
       "max        20.007208      15.594995  ...      27.202839       8.361985   \n",
       "\n",
       "                 V23            V24            V25            V26  \\\n",
       "count  142404.000000  142404.000000  142404.000000  142404.000000   \n",
       "mean       -0.000891       0.001056      -0.001327      -0.001171   \n",
       "std         0.613224       0.603888       0.521120       0.481864   \n",
       "min       -32.828995      -2.836627      -8.696627      -2.604551   \n",
       "25%        -0.162090      -0.353028      -0.317476      -0.327046   \n",
       "50%        -0.011382       0.040610       0.014350      -0.053482   \n",
       "75%         0.148334       0.438298       0.349133       0.240137   \n",
       "max        22.083545       4.016342       6.070850       3.517346   \n",
       "\n",
       "                 V27            V28         Amount          Class  \n",
       "count  142404.000000  142404.000000  142404.000000  142404.000000  \n",
       "mean       -0.000020      -0.000314      88.454103       0.001671  \n",
       "std         0.399919       0.318633     246.907421       0.040848  \n",
       "min       -22.565679     -11.710896       0.000000       0.000000  \n",
       "25%        -0.071220      -0.053085       5.630000       0.000000  \n",
       "50%         0.001197       0.011287      21.980000       0.000000  \n",
       "75%         0.090985       0.078742      77.000000       0.000000  \n",
       "max        12.152401      33.847808   18910.000000       1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Time', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.900622579042403,\n",
      "0.280587861664009,\n",
      "-1.16418507445319,\n",
      "-0.479059833649251,\n",
      "2.3064567073629303,\n",
      "-1.10981977886077,\n",
      "2.09116319137464,\n",
      "-1.01413490027071,\n",
      "-0.163032171927941,\n",
      "-0.295286482355229,\n",
      "-0.6298151244797501,\n",
      "-0.342882001890299,\n",
      "0.36842732543323,\n",
      "-1.4897687211888102,\n",
      "-0.5022359245827679,\n",
      "-0.366396749156911,\n",
      "0.405848024644247,\n",
      "-0.145480303734502,\n",
      "-0.0568360635252561,\n",
      "-0.2952823196352719,\n",
      "-0.0287383248838232,\n",
      "0.703405767865118,\n",
      "0.0779631730764966,\n",
      "0.4105065423993879,\n",
      "-0.203357971544653,\n",
      "0.492113918404666,\n",
      "-0.5262416695651839,\n",
      "-0.0858909981161408,\n",
      "62.34,\n"
     ]
    }
   ],
   "source": [
    "for i in np.array(df.drop('Class', axis=1).iloc[0]): print(f'{i},')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113923, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    113733\n",
       "1       190\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28481, 30)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    28433\n",
       "1       48\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_test.shape)\n",
    "df_test['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Sklearn Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=4, n_jobs=10)\n",
    "\n",
    "model.fit(df_train.drop('Class', axis=1), df_train['Class'])\n",
    "\n",
    "pred_train_prob = model.predict_proba(df_train.drop('Class', axis=1))\n",
    "pred_test_prob = model.predict_proba(df_test.drop('Class', axis=1))\n",
    "\n",
    "pred_train = model.predict(df_train.drop('Class', axis=1))\n",
    "pred_test = model.predict(df_test.drop('Class', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df, pred):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    unique_labels = df['Class'].unique()\n",
    "    N = len(unique_labels)\n",
    "    confusion = confusion_matrix(df['Class'], pred)\n",
    "    ax.matshow(np.log(confusion + 1.001))\n",
    "\n",
    "    ax.set_xticks(range(N))\n",
    "    ax.set_yticks(range(N))\n",
    "\n",
    "    ax.set_xticklabels(unique_labels)\n",
    "    ax.set_yticklabels(unique_labels)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(N):        \n",
    "            ax.text(j, i, confusion[i,j], va='center', ha='center')\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Labels')\n",
    "\n",
    "plot_confusion_matrix(df_train, pred_train);\n",
    "plot_confusion_matrix(df_test, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(df, pred_prob, label):\n",
    "    precision, recall, thresholds = precision_recall_curve(df['Class'], pred_prob[:,1])\n",
    "    plt.plot(precision, recall, 'p-', label=label)\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Precision vs Recall Trade-off Curve')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    \n",
    "plot_precision_recall_curve(df_train, pred_train_prob, 'Train')\n",
    "plot_precision_recall_curve(df_test, pred_test_prob, 'Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PyTorch Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_hidden = 128\n",
    "N_output = 1\n",
    "\n",
    "net = nn.Sequential(nn.Linear(df_train.shape[1]-1, N_hidden),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(N_hidden, N_output),\n",
    "                    #nn.Sigmoid()\n",
    "                   )\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "        \n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "ds_torch_train = CCDataset(df_train)\n",
    "ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "dl_torch_train = DataLoader(ds_torch_train, batch_size=128, num_workers=0)\n",
    "dl_torch_test = DataLoader(ds_torch_test, batch_size=128, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_torch_train[0])\n",
    "print(df_train.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, 10, 1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEED TO DEBUG THIS\n",
    "#criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([1., 1000.]))\n",
    "#model, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion_weighted, 10, 1, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export to Script\n",
    "\n",
    "For many tools, hyperparameter tuning using Katib for example, we will need a script that can accept command line arguments and package it in an image. Since many data scientists prefer using Jupyter notebooks, we'll add this section to write a main function and convert it to a script. An alternative is to do your development work in scripts and import the scripts in a notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single node script\n",
    "\n",
    "The cell below writes the contents to a file called ccfraud.py. It doesn't execute the contents in the cell.\n",
    "\n",
    "The training is done on a single physical node with the only parallelization possible being one across threads on the processor on the node and on an attached GPU.\n",
    "\n",
    "For parallelization by splitting the data (but not the model) across multiple physical nodes, please see the next example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ccfraud.py\n",
    "\n",
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_architecture(df_train, N_hidden=128):\n",
    "    '''Wrapper around the neural net architecture\n",
    "    '''\n",
    "    N_output = 1\n",
    "\n",
    "    net = nn.Sequential(nn.Linear(df_train.shape[1]-1, N_hidden),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(N_hidden, N_output),\n",
    "                        #nn.Sigmoid()\n",
    "                       )\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def get_criterion(weighted=False, pos_weight=torch.tensor([1,1])):\n",
    "    '''Wrapper around criterion\n",
    "    '''\n",
    "    if not weighted:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) #NEEDS DEBUGGING\n",
    "        \n",
    "    return criterion\n",
    "        \n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "\n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    \n",
    "\n",
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3, optimizer='adam'):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if optimizer=='adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Using adam\")\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        print(\"Using sgd\")\n",
    "    else:\n",
    "        raise ValueError(\"Please use either adam or sgd\")\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Credit Card Fraud Detection')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='batch size for training (default = 64)')\n",
    "\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, metavar='N',\n",
    "                        help='number of nodes in hidden layers')\n",
    "\n",
    "    parser.add_argument('--optimizer', type=str, default='adam', metavar='N',\n",
    "                        help='optimizer to use: \"adam\" or \"sgd\"')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    \n",
    "    parser.add_argument('--dir', default='logs', metavar='L',\n",
    "                        help='directory where summary logs are stored')\n",
    "  \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    N_epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    N_print = args.log_interval\n",
    "    N_hidden = args.n_hidden\n",
    "    optimizer = args.optimizer\n",
    "    \n",
    "    #Read data and preprocessing\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    ds_torch_train = CCDataset(df_train)\n",
    "    ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "    dl_torch_train = DataLoader(ds_torch_train, batch_size=batch_size, num_workers=0)\n",
    "    dl_torch_test = DataLoader(ds_torch_test, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    #Network architecture and criterion\n",
    "    net = get_architecture(df_train, N_hidden=N_hidden)\n",
    "    criterion = get_criterion()\n",
    "    print(net)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    net, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, N_epochs, N_print, lr=lr, optimizer=optimizer)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple nodes script\n",
    "\n",
    "The cell below writes the contents to a file called ccfraud_distributed.py. It doesn't execute the contents in the cell.\n",
    "\n",
    "The training is done across multiple physical nodes. torch.distributed has a range of options for distributing across both a single physical node with multiple GPUs as well as across multiple physical nodes either by having each node compute gradients on a fraction of the data or even by having subsets of the model across different nodes.\n",
    "\n",
    "Example: https://github.com/kubeflow/pytorch-operator/blob/master/examples/mnist/mnist.py\n",
    "\n",
    "torch.distributed: start here https://pytorch.org/tutorials/beginner/dist_overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ccfraud_distributed.py\n",
    "\n",
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.distributed as dist\n",
    "\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "WORLD_SIZE = int(os.environ.get('WORLD_SIZE', 1))\n",
    "print(f'WORLD_SIZE = {WORLD_SIZE}')\n",
    "\n",
    "def get_architecture(df_train, N_hidden=128):\n",
    "    '''Wrapper around the neural net architecture\n",
    "    '''\n",
    "    N_output = 1\n",
    "\n",
    "    net = nn.Sequential(nn.Linear(df_train.shape[1]-1, N_hidden),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(N_hidden, N_output),\n",
    "                        #nn.Sigmoid()\n",
    "                       )\n",
    "    \n",
    "    return net\n",
    "\n",
    "\n",
    "def get_criterion(weighted=False, pos_weight=torch.tensor([1,1])):\n",
    "    '''Wrapper around criterion\n",
    "    '''\n",
    "    if not weighted:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) #NEEDS DEBUGGING\n",
    "        \n",
    "    return criterion\n",
    "        \n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "\n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    \n",
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3, optimizer='adam'):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if optimizer=='adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Using adam\")\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        print(\"Using sgd\")\n",
    "    else:\n",
    "        raise ValueError(\"Please use either adam or sgd\")\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision\n",
    "\n",
    "def should_distribute():\n",
    "    return dist.is_available() and WORLD_SIZE > 1\n",
    "    \n",
    "def is_distributed():\n",
    "    return dist.is_available() and dist.is_initialized()\n",
    "    \n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Credit Card Fraud Detection')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='batch size for training (default = 64)')\n",
    "\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, metavar='N',\n",
    "                        help='number of nodes in hidden layers')\n",
    "\n",
    "    parser.add_argument('--optimizer', type=str, default='adam', metavar='N',\n",
    "                        help='optimizer to use: \"adam\" or \"sgd\"')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    \n",
    "    parser.add_argument('--dir', default='logs', metavar='L',\n",
    "                        help='directory where summary logs are stored')\n",
    "  \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    N_epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    N_print = args.log_interval\n",
    "    N_hidden = args.n_hidden\n",
    "    optimizer = args.optimizer\n",
    "        \n",
    "    #distributed\n",
    "    if should_distribute():\n",
    "        dist.init_process_group(dist.Backend.GLOO)\n",
    "        print('Using distributed PyTorch with backend GLOO')\n",
    "    \n",
    "    #Read data and preprocessing\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    ds_torch_train = CCDataset(df_train)\n",
    "    ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "    dl_torch_train = DataLoader(ds_torch_train, batch_size=batch_size, num_workers=0)\n",
    "    dl_torch_test = DataLoader(ds_torch_test, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    #Network architecture and criterion\n",
    "    net = get_architecture(df_train, N_hidden=N_hidden)\n",
    "    criterion = get_criterion()\n",
    "    print(net)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    if is_distributed():\n",
    "        Distributor = nn.parallel.DistributedDataParallelCPU\n",
    "        net = Distributor(net)\n",
    "    \n",
    "    net, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, N_epochs, N_print, lr=lr, optimizer=optimizer)\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Single Node Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ccfraud_serving.py\n",
    "\n",
    "import os, boto3, time, operator, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "\n",
    "#Scikit-learn imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve,\\\n",
    "                            average_precision_score,\\\n",
    "                            roc_auc_score, roc_curve,\\\n",
    "                            confusion_matrix, classification_report\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "#PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, df_train, N_hidden=128):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.N_input = df_train.shape[1]-1\n",
    "        self.N_output = 1\n",
    "        \n",
    "        self.layer1 = nn.Linear(self.N_input, N_hidden)\n",
    "        self.layer2 = nn.Linear(N_hidden, self.N_output)\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer2(self.act(self.layer1(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "def get_criterion(weighted=False, pos_weight=torch.tensor([1,1])):\n",
    "    '''Wrapper around criterion\n",
    "    '''\n",
    "    if not weighted:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) #NEEDS DEBUGGING\n",
    "        \n",
    "    return criterion\n",
    "        \n",
    "class CCDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.N_cols = df.shape[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, ix):\n",
    "        x = np.array(self.df.iloc[ix])\n",
    "        features = x[:(self.N_cols-1)] #exclude time, Class\n",
    "        label = x[[-1]]\n",
    "\n",
    "        #return {'features': torch.from_numpy(features), 'label': torch.from_numpy(label)}\n",
    "        return (torch.from_numpy(features).float(), torch.from_numpy(label))\n",
    "    \n",
    "\n",
    "def train_model(train_dl, test_dl, model, criterion, N_epochs, print_freq, lr=1e-3, optimizer='adam'):\n",
    "    '''Loop over dataset in batches, compute loss, backprop and update weights\n",
    "    '''\n",
    "    \n",
    "    model.train() #switch to train model (for dropout, batch normalization etc.)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    if optimizer=='adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        print(\"Using adam\")\n",
    "    elif optimizer=='sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "        print(\"Using sgd\")\n",
    "    else:\n",
    "        raise ValueError(\"Please use either adam or sgd\")\n",
    "    \n",
    "    avg_precision_dict, loss_dict = {}, {}\n",
    "    for epoch in range(N_epochs): #loop over epochs i.e. sweeps over full data\n",
    "        curr_loss = 0\n",
    "        N = 0\n",
    "        \n",
    "        for idx, (features, labels) in enumerate(train_dl): #loop over batches\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            preds = model(features)\n",
    "            loss = criterion(preds.squeeze(), labels.squeeze().float())\n",
    "            \n",
    "            curr_loss += loss.item() #accumulate loss\n",
    "            N += len(labels) #accumulate number of data points seen in this epoch\n",
    "                \n",
    "            #backprop and updates\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if epoch % print_freq == 0 or epoch==N_epochs-1:\n",
    "            val_loss, val_avg_precision = validate(test_dl, model, criterion) #get model perf metrics from test set\n",
    "            \n",
    "            avg_precision_dict[epoch] = val_avg_precision\n",
    "            loss_dict[epoch] = val_loss\n",
    "            \n",
    "            print(f'Iter = {epoch} Train Loss = {curr_loss / N} val_loss = {val_loss} val_avg_precision = {val_avg_precision}')\n",
    "            \n",
    "    return model, avg_precision_dict, loss_dict\n",
    "\n",
    "def validate(test_dl, model, criterion):\n",
    "    '''Loop over test dataset and compute loss and accuracy\n",
    "    '''\n",
    "    model.eval() #switch to eval model\n",
    "    \n",
    "    loss = 0\n",
    "    N = 0\n",
    "\n",
    "    preds_all, labels_all = torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    with torch.no_grad(): #no need to keep variables for backprop computations\n",
    "        for idx, (features, labels) in enumerate(test_dl):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device).float()\n",
    "            \n",
    "            preds = model(features)\n",
    "            \n",
    "            preds_all = torch.cat((preds_all, preds.to('cpu')), 0)\n",
    "            labels_all = torch.cat((labels_all, labels.to('cpu')), 0)\n",
    "            \n",
    "            loss += criterion(preds.squeeze(), labels.squeeze()) #cumulative loss\n",
    "            N += len(labels)\n",
    "    \n",
    "    avg_precision = average_precision_score(labels_all.squeeze().numpy(), preds_all.squeeze().numpy())\n",
    "    \n",
    "    return loss / N, avg_precision\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Credit Card Fraud Detection')\n",
    "    \n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='batch size for training (default = 64)')\n",
    "\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 10)')\n",
    "\n",
    "    parser.add_argument('--n-hidden', type=int, default=16, metavar='N',\n",
    "                        help='number of nodes in hidden layers')\n",
    "\n",
    "    parser.add_argument('--optimizer', type=str, default='adam', metavar='N',\n",
    "                        help='optimizer to use: \"adam\" or \"sgd\"')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=1e-3, metavar='LR',\n",
    "                        help='learning rate (default: 1e-3)')\n",
    "\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    \n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    \n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "    \n",
    "    parser.add_argument('--dir', default='logs', metavar='L',\n",
    "                        help='directory where summary logs are stored')\n",
    "  \n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    N_epochs = args.epochs\n",
    "    lr = args.lr\n",
    "    N_print = args.log_interval\n",
    "    N_hidden = args.n_hidden\n",
    "    optimizer = args.optimizer\n",
    "    \n",
    "    #Read data and preprocessing\n",
    "    df = pd.read_csv('creditcard.csv')\n",
    "    df.drop('Time', inplace=True, axis=1)\n",
    "\n",
    "    # Train-test split\n",
    "    df_train, df_test = train_test_split(df, train_size=0.8)\n",
    "\n",
    "    ds_torch_train = CCDataset(df_train)\n",
    "    ds_torch_test = CCDataset(df_test)\n",
    "\n",
    "    dl_torch_train = DataLoader(ds_torch_train, batch_size=batch_size, num_workers=0)\n",
    "    dl_torch_test = DataLoader(ds_torch_test, batch_size=batch_size, num_workers=0)\n",
    "\n",
    "    #Network architecture and criterion\n",
    "    net = Net(df_train, N_hidden=N_hidden)\n",
    "    criterion = get_criterion()\n",
    "    print(net)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    \n",
    "    net, avg_precision_dict, loss_dict = train_model(dl_torch_train, dl_torch_test, net, criterion, N_epochs, N_print, lr=lr, optimizer=optimizer)\n",
    "\n",
    "    net.eval()\n",
    "    torch.save(net.state_dict(), \"model.pt\")\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import operator\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "feat_imp = sorted(zip(features_train_pd.columns, model.feature_importances_), key=operator.itemgetter(1), reverse=True)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot([i[0] for i in feat_imp], [i[1] for i in feat_imp], 'p-')\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-create the model with Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define features and target variables for convenience.\n",
    "## From the graph we only want seven important features V3,V4,V10,V11,V12,V14,V17\n",
    "drop_time_class = ['_c0', 'Time', 'Class','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28']\n",
    "drop_class=['Class']\n",
    "\n",
    "\n",
    "features_train = df_train.drop(*drop_time_class)\n",
    "target_train = df_train.select(\"Class\")\n",
    "\n",
    "features_test = df_test.drop(*drop_time_class)\n",
    "target_test = df_test.select(\"Class\")\n",
    "features_test.printSchema()\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, max_depth=6, n_jobs=10, class_weight='balanced')\n",
    "                               \n",
    "#Convert to pandas\n",
    "features_test_pd = features_test.toPandas()\n",
    "target_test_pd = target_test.toPandas()\n",
    "\n",
    "features_train_pd = features_train.toPandas()\n",
    "target_train_pd = target_train.toPandas()\n",
    "\n",
    "model.fit(features_train_pd, target_train_pd.values.ravel())\n",
    "\n",
    "pred_train = model.predict(features_train_pd)\n",
    "pred_test = model.predict(features_test_pd)\n",
    "\n",
    "pred_train_prob = model.predict_proba(features_train_pd)\n",
    "pred_test_prob = model.predict_proba(features_test_pd)\n",
    "\n",
    "print(\"Number of features\")\n",
    "print(len(model.feature_importances_))\n",
    "  \n",
    "#save mode in filesystem\n",
    "joblib.dump(model, 'model.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_confusion_matrix(target_train_pd, model.predict(features_train_pd))\n",
    "\n",
    "_ = plot_confusion_matrix(target_test_pd, model.predict(features_test_pd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    rowdf = pd.DataFrame([data.tolist()], columns = ['V3','V4','V10','V11','V12','V14','V17','Amount'])\n",
    "    print(model.predict(rowdf))\n",
    "    time.sleep(2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model to Rook/Ceph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "key = \"uploaded/model.pkl\"\n",
    "s3.upload_file(Bucket=s3_bucket, Key=key, Filename=\"model.pkl\")\n",
    "prefix='uploaded/'\n",
    "result = s3.list_objects(Bucket=s3_bucket, Prefix=prefix, Delimiter='/')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install OpenShift client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -o oc.tar.gz -L https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/4.8.5/openshift-client-linux-4.8.5.tar.gz\n",
    "tar xzf oc.tar.gz\n",
    "cp oc ~/../bin/oc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login into Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "oc login -u admin -p opennaru --insecure-skip-tls-verify https://api.openmaru.ocp482.com:6443\n",
    "oc project frauddetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serve Model With Seldon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "oc project frauddetection\n",
    "oc create -n frauddetection -f https://raw.githubusercontent.com/nakfour/odh-kubeflow/master/mymodel.json\n",
    "oc get seldondeployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model in Curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -o jq -L https://github.com/stedolan/jq/releases/download/jq-1.6/jq-linux64\n",
    "cp jq ~/../bin/jq\n",
    "chmod 777 ~/../bin/jq\n",
    "export TOKENJSON=$(curl -XPOST -u oauth-key:oauth-secret <INSERT SELDON API SERVER URL>/oauth/token -d 'grant_type=client_credentials')\n",
    "export TOKEN=$(echo $TOKENJSON | jq \".access_token\" -r)\n",
    "echo $TOKEN\n",
    "\n",
    "curl -v --header \"Authorization: Bearer $TOKEN\" <INSERT SELDON API SERVER URL>/api/v0.1/predictions -d '{\"strData\": \"0.365194527642578,0.819750231339882,-0.5927999453145171,-0.619484351930421,-2.84752569239798,1.48432160780265,0.499518887687186,72.98\"}' -H \"Content-Type: application/json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Served Full Model In Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing the served model from python using the test dataframe\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Get the token\n",
    "post_data = {\"grant_type\": \"client_credentials\"}\n",
    "requestOauth = requests.post('<INSERT SELDON API SERVER URL>/oauth/token', auth=('oauth-key', 'oauth-secret'), data=post_data, json={'grant_type=client_credentials'})\n",
    "\n",
    "data = requestOauth.json();\n",
    "print(data['access_token'])\n",
    "access_token = data['access_token']\n",
    "\n",
    "headers = {'Content-type': 'application/json', 'Authorization': 'Bearer {}'.format(access_token)}\n",
    "#Read the test dataframe and stream each row\n",
    "df_test_pandas = df_test.toPandas()\n",
    "fraudTest = df_test_pandas.loc[df_test_pandas['Class']== 1]\n",
    "notFraudTest = df_test_pandas.loc[df_test_pandas['Class']== 0]\n",
    "\n",
    "fraudTestFeatures = fraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "notFraudTestFeatures = notFraudTest.drop(columns=['Time','Class', '_c0','V1','V2','V5','V6','V7','V8','V9','V13','V15','V16','V18','V19','V20','V21','V22','V23','V24','V25','V26','V27','V28'])\n",
    "#for index, row in features_test.toPandas().iterrows():\n",
    "for index, row in fraudTestFeatures.iterrows():\n",
    "    data = row\n",
    "    str1 = ','.join(str(e) for e in  data)\n",
    "    requestPrediction = requests.post('<INSERT SELDON API SERVER URL>/api/v0.1/predictions', headers=headers, json={\"strData\": str1 })\n",
    "    predictionData = requestPrediction.json();\n",
    "    datafield = predictionData['data']\n",
    "    predictionArray = datafield['ndarray']\n",
    "    print(predictionArray[0])\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%bash\n",
    "#oc project frauddetection\n",
    "#oc delete seldondeployments mymodel\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
